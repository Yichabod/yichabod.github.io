<!DOCTYPE html>
<html lang=""><head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title></title>
    <meta name="description" content="Max&#39;s personal website">
    <meta name="author" content='Max Langenkamp'>

    <link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" integrity="sha512-iBBXm8fW90+nuLcSKlbmrPcLa0OT92xO1BIsZ+ywDWZCvqsWgccV3gFoRBv0z+8dLJgyAHIhR35VZc2oM/gI1w==" crossorigin="anonymous">

    
    <link rel="stylesheet" href="/sass/researcher.min.css">

    
        <link rel="icon" type="image/ico" href="https://yichabod.github.io/favicon.ico">
    

    
        
    
</head>

    <body><div class="container mt-5">
    <nav class="navbar navbar-expand-sm flex-column flex-sm-row text-nowrap p-0">
        <a class="navbar-brand mx-0 mr-sm-auto" href="https://yichabod.github.io" title="Max Langenkamp">
          
          Max Langenkamp
        </a>
        <div class="navbar-nav flex-row flex-wrap justify-content-center">
            
                
                
                    <a class="nav-item nav-link" href="/writing" title="writing">
                        writing
                    </a>
                    
                        <span class="nav-item navbar-text mx-1">/</span>
                    
                
                    <a class="nav-item nav-link" href="/projects" title="projects">
                        projects
                    </a>
                    
                        <span class="nav-item navbar-text mx-1">/</span>
                    
                
                    <a class="nav-item nav-link" href="https://maxlangenkamp.substack.com/" title="newsletter">
                        newsletter
                    </a>
                    
                        <span class="nav-item navbar-text mx-1">/</span>
                    
                
                    <a class="nav-item nav-link" href="https://twitter.com/mslkmp" title="twitter">
                        twitter
                    </a>
                    
                
            
        </div>
    </nav>
</div>
<hr>
<div id="content">
<div class="container">
    <h1 id="technology-is-not-inevitable">Technology is not inevitable</h1>
<p>When talking to certain technologist friends about brain-computer interfaces and artificial general intelligence, I’m struck by how often I hear that such idea-complexes<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> are referred to as ‘inevitable’.</p>
<p><img src="/inevitable.png" alt="alt_text" title="image_tooltip"></p>
<p>In this example, my friend (who I’ll use to represent the various people<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> who’ve held this stance) basically says: ‘if it is physically possible, and there is economic incentive, it’ll happen’. Let’s put aside the initial question about the ambiguity of what counts as BCI/AGI.</p>
<p>I’m going to argue that claims about technological inevitability are wrong and monopolistic.</p>
<h3 id="technological-feasibility-and-economic-incentives-do-not-imply-inevitability">Technological feasibility and economic incentives do not imply inevitability</h3>
<p>When I say ‘BCI is inevitable’ is wrong, I mean that this is a misleadingly strong prediction about the future. I don’t mean ‘BCI will not happen’. Nor, am i suggesting that the future is inherently nondeterministic.</p>
<p>Here are a few examples of technologies that are both physically possible and economically incentivized yet have either been greatly delayed or outright banned. From Katja Grace’s “<a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiSwvy-z739AhUHElkFHXnfCx4QFnoECAwQAQ&amp;url=https%3A%2F%2Faiimpacts.org%2Flets-think-about-slowing-down-ai%2F&amp;usg=AOvVaw0IqB5K77YTuMHS2SfK4pLk">Let’s talk about slowing down AI</a>”:</p>
<ol>
<li>Huge amounts of medical research, including really important medical research e.g. The FDA<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7326309/#:~:text=Nonetheless%2C%20in%201978%20the%20controversy%20resulted%20in%20a%20US%20FDA%20ban%20on%20subsequent%20vaccine%20trials%20which%20was%20eventually%20overturned%2030%20years%20later."> banned</a> human trials of strep A vaccines from the 70s to the 2000s, in spite of<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6474463/#:~:text=Worldwide%2C%20the%20death%20toll%20is%20estimated%20at%20500%20000%20annually"> 500,000 global deaths every year</a>. A lot of people also died while covid vaccines went through all the proper trials.</li>
<li>Nuclear energy</li>
<li>Fracking</li>
<li>Various genetics things: genetic modification of foods, gene drives, early recombinant DNA researchers famously organized a moratorium and then ongoing research guidelines including prohibition of certain experiments (see the<a href="https://en.wikipedia.org/wiki/Asilomar_Conference_on_Recombinant_DNA"> Asilomar Conference</a>)</li>
<li>Nuclear, biological, and maybe chemical weapons (or maybe these just aren’t useful)</li>
<li>Various human reproductive innovation: cloning of humans, genetic manipulation of humans (a notable example of an economically valuable technology that is to my knowledge barely pursued across different countries, without explicit coordination between those countries, even though it would make those countries more competitive. Someone used CRISPR on babies in China, but was<a href="https://www.science.org/content/article/creator-crispr-babies-nears-release-prison-where-does-embryo-editing-stand"> imprisoned</a> for it.)</li>
<li>Recreational drug development</li>
<li>Geoengineering…</li>
</ol>
<p>From the 60s to the 80s, people really believed that nuclear power was the future. As late as 1987, Asimov thought that nuclear fusion would be widespread by 2020<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. Many were similarly confident about nuclear energy<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. Even though nuclear energy made sense technologically and economically, public opinion has turned against it after a series of memorable disasters<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>, indefinitely stalling progress.</p>
<p>Perhaps a stronger example is genetic modification of humans. Countries could have workshops with human-like clones but genetically modified to align lithography photomasks to more quickly create semiconductors. We don’t see anything close to this because people like Federico Mayor<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> helped create consensus around the <a href="https://www.unesco.org/en/ethics-science-technology/human-genome-and-human-rights">Universal Declaration on the Human Genome and Human Rights</a>.</p>
<p>Alongside flying cars and subsonic intercontinental travel, human genetic modification remains a specter in the distance. It may eventually happen, but any claims about its inevitability are not well substantiated.</p>
<h3 id="technology-is-inevitable-can-be-dangerously-self-fulfilling">“Technology is inevitable” can be dangerously self-fulfilling</h3>
<p>We deform the world according to our expectations. When this happens, we call it a ‘self-fulfilling prophecy’. The world is more deeply filled with self-fulfilling prophecies than we recognize, so we should be very careful about our ‘prophecies’, lest they crowd out visions of the future we want.</p>
<p>Consider stock-market crashes. “Short-sellers” can cause companies to go bankrupt by publicizing a persuasive case for why a particular company should not be valued so highly. If the short-seller is reputable, the stock price will crash within the next few days<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. <strong>One way of framing this is that the narrative power has shifted from the company</strong> (“we at E-corp are an honest and profitable business”) <strong>to the short-seller</strong> (“E-corp is a pyramid scheme that rests on fraud”). The fundamental economics of the companies remain the same, but the shift in narrative power can convince shareholders to sell and drive the company bankrupt<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>.</p>
<p>Let’s turn back to AGI. The conception of “AGI” in the narrative that “AGI is inevitable” is part of a fairly specific vision of AI. It evokes paperclip maximizers, agents with utility functions, a <a href="https://betterwithout.ai/artificial-general-intelligence">monstrous superintelligence</a>. Because its proponents were early and vocal, <strong>“AGI” as a frame has monopolized the narrative power</strong>. This has crowded out other visions of AI systems that are both technologically feasible and more socially desirable<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>. Drexler’s ‘Comprehensive AI Services’, for instance, does not involve a power-seeking superintelligent agent<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>.</p>
<p>The future is less certain than we realize, and a wise course of action involves acknowledging both our uncertainty but also the way that narrative power can shape outcomes. Be wary the oracle bearing the message that their technology is inevitable.</p>
<hr>
<p><em>For inspiring and refining this essay, I thank Raffi Hotter, Saffron Huang, Henry Williams, and Helen Toner.</em></p>
<!-- Footnotes themselves at the bottom. -->
<h4 id="footnotes">Footnotes</h4>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>I think it makes more sense to refer to BCI/AGI as big parts of an idea machine, to use Nadia Aspourahova’s term, rather than a distinct technology.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>See, for instance, Jeff Clune at ~45:00 in <a href="https://twimlai.com/podcast/twimlai/accelerating-intelligence-with-ai-generating-algorithms/">https://twimlai.com/podcast/twimlai/accelerating-intelligence-with-ai-generating-algorithms/</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>“nuclear fusion [will] offer a controlled and practical source of unlimited energy”. See “The Elevator Effect” from <a href="https://archive.org/stream/66EssaysOnThePastPresentAndFuture/IsaacAsimov-66EssaysOnThePastPresentFuture-barnesNobleInc1993_djvu.txt">https://archive.org/stream/66EssaysOnThePastPresentAndFuture/IsaacAsimov-66EssaysOnThePastPresentFuture-barnesNobleInc1993_djvu.txt</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>See Karnofsky <a href="https://www.cold-takes.com/the-track-record-of-futurists-seems-fine/">https://www.cold-takes.com/the-track-record-of-futurists-seems-fine/</a> and the informative response from Dan Luu <a href="https://danluu.com/futurist-predictions/">https://danluu.com/futurist-predictions/</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p>Though I haven’t read it, Weart’s ‘The Rise of Nuclear Fear’ is apparently very good on this.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6" role="doc-endnote">
<p>See <a href="https://www.researchgate.net/publication/26402030_The_Significance_of_UNESCO%27s_Universal_Declaration_on_the_Human_Genome_and_Human_Rights">https://www.researchgate.net/publication/26402030_The_Significance_of_UNESCO%27s_Universal_Declaration_on_the_Human_Genome_and_Human_Rights</a>&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7" role="doc-endnote">
<p>This happened with <a href="https://www.investopedia.com/news/billionaire-bill-ackman-dumps-herbalife-ending-5year-war-betting-against-it/">Bill Ackman and Herbalife</a>, the pyramid scheme pharmaceutical company. Hindenburg Research’s report on the Adani group caused a <a href="https://www.theguardian.com/business/2023/feb/02/why-has-the-adani-group-shed-us90bn-in-value-and-what-do-short-sellers-have-to-gain">$90bn decrease</a> in market value across their companies.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8" role="doc-endnote">
<p>We don’t realize that this is happening all the time. Hype cycles, the placebo, the ‘pygmalion effect’, stereotypes, predictive policing, the CAPM pricing model#. You might say that every moment, we are collectively sustaining and hallucinating hyperobjects into existence.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9" role="doc-endnote">
<p>See, for instance, Drexler’s ‘Comprehensive AI Services’ and ‘Open Agencies’, and Audrey Tang’s ‘Digital Pluralism’. Also Eckersley on the analogy between capitalism and gradient descent and Chiang’s metaphor of AI as a corporation.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10" role="doc-endnote">
<p>There are many debates over whether AI will “really be” <a href="https://gwern.net/tool-ai">an agent or a tool</a> (see e.g. Gwern’s “Why tool AIs want to be agent AIs”), but these discussions miss the self-fulfilling power of narratives.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

</div>

        </div><div id="footer" class="mb-5">
    <hr>
    <div class="container text-center">
        
            <a href="https://twitter.com/mslkmp" class="fab fa-twitter fa-1x" title="Twitter"></a>
        
            <a href="https://www.linkedin.com/in/max-langenkamp-2aa6b435/" class="fab fa-linkedin fa-1x" title="LinkedIn"></a>
        
            <a href="mailto:maxlangenkamp@me.com" class="fas fa-envelope fa-1x" title="E-mail"></a>
        
    </div>
    
        <div class="container text-center">
            <a href="https://github.com/ojroques/hugo-researcher" title="By Max Langenkamp"><small>By Max Langenkamp</small></a>
        </div>
    
</div>
</body>
</html>
